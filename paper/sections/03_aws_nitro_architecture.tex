\section{AWS Nitro Enclaves Architecture}

Amazon Web Services Nitro Enclaves represents a distinctive implementation of Trusted Execution Environment technology that leverages the Nitro System, AWS's custom hypervisor and hardware security architecture. Unlike processor-specific TEE implementations that depend on features from particular CPU vendors, Nitro Enclaves operates at the hypervisor level to provide isolation guarantees across diverse processor architectures including Intel, AMD, and AWS-designed Graviton processors based on ARM architecture. This platform-agnostic approach enables consistent security properties regardless of underlying hardware, simplifying deployment for organizations operating multi-architecture infrastructure.

The Nitro System itself evolved from AWS's strategic decision to disaggregate traditional virtualization stacks into specialized hardware and software components. Rather than relying on complex general-purpose hypervisors running on host CPUs, the Nitro architecture offloads virtualization functions including network, storage, and management operations to dedicated Nitro Cards implemented as custom hardware. This disaggregation reduces the trusted computing base for the hypervisor while simultaneously improving performance by removing virtualization overhead from the critical path of guest computation. Nitro Enclaves extends this architectural philosophy to provide isolated execution environments with minimal dependencies on host software.

The security model of Nitro Enclaves derives from several architectural principles that distinguish it from other TEE implementations. First, enclaves have no external network connectivity, eliminating entire classes of network-based attacks and preventing data exfiltration through network channels. Communication occurs exclusively through a secure local socket connection to the parent EC2 instance, creating a well-defined and auditable interface. Second, enclaves cannot be accessed interactively via SSH or any other remote access mechanism, preventing administrative access that could compromise security. Third, enclaves have no persistent storage, ensuring that sensitive data does not persist after enclave termination. These constraints create a highly restricted execution environment optimized for processing sensitive data with minimal attack surface.

The practical implications of these design decisions affect both security properties and operational characteristics. The lack of external networking simplifies security analysis by reducing the threat surface but requires careful design of communication patterns between enclaves and external systems. Applications must proxy all external communication through the parent instance, which handles network operations on behalf of the enclave. The absence of persistent storage means that enclaves must reload all data and code on each instantiation, affecting startup times but ensuring clean state initialization. The prohibition on interactive access prevents operators from debugging running enclaves directly, necessitating alternative approaches to troubleshooting based on logging and monitoring through the parent instance interface.

\subsection{Nitro Hypervisor and Isolation Mechanisms}

The Nitro Hypervisor provides the foundational isolation layer that enables secure enclave operation. Built as a lightweight, purpose-designed hypervisor rather than a general-purpose virtualization platform, the Nitro Hypervisor focuses specifically on security and performance rather than feature richness. This minimalist design philosophy reduces complexity and shrinks the trusted computing base compared to traditional hypervisors that support diverse workloads and extensive management interfaces.

The hypervisor enforces CPU isolation by dedicating specific processor cores exclusively to enclave execution. When an enclave is created, the hypervisor allocates one or more complete CPU cores from the parent EC2 instance to the enclave. These cores become unavailable to the parent instance for the duration of the enclave's lifetime, ensuring complete separation of execution contexts. The hypervisor does not schedule both enclave and parent workloads on the same core, eliminating classes of cache-based side channel attacks that could otherwise leak information between contexts. This core-level isolation represents a stronger guarantee than time-sliced scheduling where multiple security domains share processor resources.

Memory isolation operates through similar principles of physical separation rather than software-mediated access control. The hypervisor allocates a contiguous region of physical memory exclusively to the enclave, removing it from the parent instance's address space. The memory management unit enforces access controls that prevent the parent instance or hypervisor from reading or writing enclave memory. Importantly, this isolation persists even when the enclave is not actively executing, protecting enclave state during context switches or power management operations. The hypervisor itself does not maintain persistent mappings to enclave memory, limiting opportunities for privileged software to bypass isolation boundaries.

The Nitro Security Module serves as the communication interface between enclaves and the hypervisor. This specialized component implements the protocols through which enclaves request services including attestation document generation, random number generation, and Platform Configuration Register operations. The NSM operates as a character device accessible within the enclave through standard file operations on the device path /dev/nsm. This interface design allows enclave applications to interact with security services using familiar POSIX semantics rather than requiring specialized APIs or system calls.

Communication between enclaves and parent instances occurs through a vsock connection, a virtual socket mechanism designed for guest-host communication in virtualized environments. The vsock protocol provides a socket-like interface for bidirectional data transfer while maintaining the isolation boundary. The hypervisor mediates this communication channel, ensuring that enclave network isolation is not compromised. The vsock connection operates as a local communication path independent of the instance's external network interfaces, preventing enclaves from establishing direct network connections that could bypass security controls. Applications typically implement request-response patterns over vsock where the parent instance receives external requests, forwards them to the enclave for processing, and returns results to external clients.

Resource allocation for enclaves is flexible within the constraints of the parent instance's capacity. When launching an enclave, operators specify the number of CPU cores and amount of memory to dedicate to the enclave. The minimum requirements vary by processor architecture, with Intel and AMD instances requiring at least four vCPUs for the parent instance to support enclaves, while Graviton-based instances require only two vCPUs. Memory allocation can range from hundreds of megabytes to tens of gigabytes depending on the parent instance type. Multiple enclaves can coexist within a single parent instance, with each receiving dedicated resources that sum to less than the instance's total capacity.

The hypervisor enforces strict isolation between concurrent enclaves on the same instance. Each enclave receives dedicated CPU cores and memory regions that are inaccessible to other enclaves and the parent instance. This multi-tenant capability enables diverse workloads to share infrastructure while maintaining security boundaries. For decentralized AI applications, this feature allows a single node operator to host multiple distinct model inference services within separate enclaves, each with independent attestation and security properties. The ability to support up to four enclaves per parent instance provides flexibility in resource utilization without compromising isolation guarantees.

\subsection{Enclave Image File Format and Components}

The Enclave Image File serves as the fundamental unit of deployment for Nitro Enclaves, encapsulating all components necessary for enclave execution in a single binary artifact. Understanding the EIF format is essential for security analysis because the cryptographic measurements used in attestation are computed from the structure and contents of this file. The format reflects AWS's design philosophy of using standard tools and formats where possible while adding security-specific extensions for measurement and verification.

The EIF structure consists of a header followed by an array of typed sections. The header contains metadata including a magic number for file type identification, version information indicating the format revision, feature flags for optional capabilities, a CRC32 checksum for integrity verification, and arrays specifying the number, sizes, and offsets of sections within the file. This header design enables parsers to navigate the file structure without requiring complete parsing of section contents, facilitating efficient validation and measurement operations.

Five distinct section types comprise an EIF file, each serving specific purposes in the enclave initialization process. The kernel section contains a bzImage format Linux kernel binary that will execute as the enclave's operating system. AWS provides pre-compiled kernel images through the aws-nitro-enclaves-sdk-bootstrap repository, though operators can theoretically compile custom kernels if they require specific features or configurations. The kernel configuration is specialized for enclave operation, including only essential drivers and features while removing unnecessary subsystems. The Linux kernel source is verified using GPG signatures from kernel maintainers including Linus Torvalds and Greg Kroah-Hartman, establishing a chain of trust to the upstream kernel project.

The cmdline section contains kernel boot command line arguments as plain text. These parameters configure kernel behavior during initialization, specifying options such as console configuration, memory allocation, and module parameters. The command line arguments affect how the kernel initializes and can influence security properties, making them part of the measured state included in attestation. Changes to command line arguments result in different Platform Configuration Register values, allowing attestation policies to enforce specific kernel configurations.

The ramdisk sections, of which there may be multiple, contain CPIO archive format file systems that provide the initial root file system for the enclave. The first ramdisk includes the init executable that serves as the enclave's initialization process, along with the NSM driver that enables communication with the Nitro Security Module. This ramdisk establishes the minimal environment necessary for the enclave to become operational. Subsequent ramdisks are generated from Docker container images provided by the operator, containing the application code and dependencies that implement the enclave's intended functionality. The linuxkit tool constructs these CPIO archives from Docker images, though AWS uses a modified version of linuxkit with undocumented changes from the upstream open source project.

The signature section contains certificate and signature pairs encoded in CBOR format. When operators sign enclave images, the signing certificates and corresponding signatures over the enclave contents are stored in this section. The signature section enables verification of enclave provenance, allowing attestation policies to restrict execution to enclaves signed by authorized parties. Unlike the other sections, the signature section content does not directly affect Platform Configuration Registers, though PCR8 is computed from signing certificates to enable attestation policies based on authorized signers.

The metadata section holds additional information encoded in CBOR format that is not included in attestation measurements. This section can contain descriptive information about the enclave such as version identifiers, build timestamps, or configuration parameters. Because metadata is excluded from PCR computation, it cannot be relied upon for security decisions during attestation. The metadata section exists primarily for operational and debugging purposes rather than security enforcement.

The process of constructing an EIF from application components involves several tools provided by AWS. Developers create Docker container images using standard containerization workflows, packaging their application code and dependencies. The nitro-cli build-enclave command orchestrates the transformation from Docker image to EIF, automatically downloading the pre-compiled kernel and init components, converting the Docker image to CPIO format using the modified linuxkit tool, combining all sections into the EIF structure, and computing the Platform Configuration Register values that will be used during attestation. This automated workflow simplifies enclave creation but introduces dependencies on AWS-provided components that become part of the trust model.

\subsection{Trust Chain and Pre-compiled Binary Analysis}

The security of Nitro Enclaves depends critically on the integrity of components included in Enclave Image Files. However, several of these components are provided as pre-compiled binaries by AWS rather than being built from source by enclave operators. This design decision reflects pragmatic trade-offs between usability and verifiability, but it creates implicit trust relationships that warrant careful examination.

The kernel binary included in EIF files originates from the aws-nitro-enclaves-sdk-bootstrap repository maintained by AWS. While the source code is available and the build process is documented, the pre-compiled binaries distributed through AWS infrastructure do not necessarily correspond exactly to the published source. Security researchers at Trail of Bits conducted hash analysis comparing pre-compiled binaries available on EC2 instances, binaries in the aws-nitro-enclaves-cli repository, and binaries freshly compiled from source code. Their analysis revealed discrepancies between these versions, indicating that the pre-compiled binaries differ from what would result from building the published source code.

These discrepancies could arise from several factors. The pre-compiled binaries might be built with different compiler versions, optimization flags, or build environment configurations than those used when compiling from published source. AWS might apply undocumented patches or modifications before building the distributed binaries. The build process might include additional steps such as binary signing or metadata injection that are not reflected in the public build scripts. Alternatively, the discrepancies could indicate more concerning scenarios such as outdated source repositories that do not reflect the actual code being deployed, or potentially compromised build infrastructure producing binaries that deviate from intended source code.

The init executable that bootstraps enclave initialization represents another pre-compiled component. This program runs as PID 1 within the enclave, performing critical initialization tasks including installing the NSM driver, mounting file systems, and pivoting to the application code contained in subsequent ramdisks. The init binary has significant privileges during the startup phase, making its integrity essential for overall security. Like the kernel, init is provided as a pre-compiled binary rather than requiring operators to build from source. The lack of reproducible builds means that operators cannot independently verify that the init binary corresponds to published source code.

The NSM driver that enables enclave communication with the Nitro Security Module is similarly pre-compiled and distributed through AWS infrastructure. This kernel module implements the interface through which applications request attestation documents, generate random numbers, and perform other security-sensitive operations. Compromise of the NSM driver could enable attacks such as forging attestation documents or leaking secrets. The driver's position between application code and hypervisor security services makes it a high-value target for potential attackers.

The modified linuxkit tool used to construct ramdisks from Docker images represents another component where AWS has made undocumented changes from the upstream open source project. The exact nature of these modifications is not publicly documented, making it difficult to assess whether they introduce security concerns or simply represent internal optimizations. The inability to reproduce EIF builds using unmodified open source tools creates dependencies on AWS's build infrastructure and processes.

These trust relationships create a dependency on AWS's software supply chain integrity. Operators building enclaves must trust that AWS has not introduced malicious code into pre-compiled components, that AWS's build infrastructure has not been compromised by external attackers, and that AWS's internal processes ensure consistency between published source code and distributed binaries. For some threat models, this trust in AWS may be acceptable, particularly when considering that AWS already controls the underlying infrastructure including the hypervisor and attestation signing keys. However, for systems aspiring to true decentralization where participants should not need to trust any single party, these implicit trust relationships represent a centralization point that contradicts the goal of trustless operation.

Mitigation strategies for these trust concerns include supporting reproducible builds where operators can compile components from source and verify they match distributed binaries. Enhanced transparency through detailed documentation of build processes, compiler settings, and any modifications to upstream source code would enable independent verification. Publication of cryptographic hashes for all distributed binaries, signed by AWS, would at least establish that specific binary versions are authentic even if their correspondence to source code cannot be verified. For truly decentralized systems, supporting multiple TEE platforms with different trust roots and enabling cross-verification between platforms could reduce dependence on any single vendor's supply chain.

\subsection{Platform Configuration Registers and Measurement}

Platform Configuration Registers serve as the cryptographic foundation for Nitro Enclaves attestation, containing measurements that uniquely identify the enclave's code, configuration, and execution environment. Understanding how PCRs are computed and what they represent is essential for implementing attestation policies and assessing security properties. The PCR mechanism draws conceptual inspiration from Trusted Platform Module specifications but adapts the model for the enclave use case.

Nitro Enclaves uses six PCR indices, each capturing measurements of specific components or attributes. PCR0 contains a measurement of the entire Enclave Image File excluding the content of individual sections. This measurement is computed as the SHA-384 hash of the EIF structure itself, including section offsets and sizes but not the actual bytes within sections. PCR0 serves as a coarse-grained identifier of the overall enclave image, changing when the structure or composition of the EIF changes but potentially remaining stable across modifications to individual section contents. The utility of PCR0 is somewhat limited compared to more specific measurements, as structural changes to the EIF are relatively rare compared to updates of kernel, application, or configuration.

PCR1 measures the kernel and boot infrastructure components. The computation involves hashing the kernel binary, command line arguments, and the first ramdisk containing the init executable and NSM driver. Specifically, PCR1 is computed as the SHA-384 hash of an initial 48-byte zero buffer concatenated with the SHA-384 hash of the concatenation of the kernel section, cmdline section, and the first ramdisk section. This measurement captures the trusted computing base provided by AWS, including the kernel, boot process, and core infrastructure. Changes to kernel versions, command line parameters, or the init infrastructure all result in different PCR1 values, allowing attestation policies to enforce specific infrastructure configurations.

PCR2 measures the application code contained in ramdisks beyond the first. These ramdisks are generated from the Docker images provided by enclave operators and contain the actual application logic that implements the enclave's purpose. PCR2 is computed as the SHA-384 hash of an initial 48-byte zero buffer concatenated with the SHA-384 hash of the second and subsequent ramdisks. This measurement provides assurance about what application code is executing within the enclave. For decentralized AI systems, PCR2 identifies the specific AI model and inference code, enabling users to verify they are interacting with the expected model rather than a substituted or modified version.

PCR3 contains a measurement of the IAM role associated with the parent EC2 instance. The value is computed as the SHA-384 hash of an initial 48-byte zero buffer concatenated with the SHA-384 hash of the IAM role ARN encoded as UTF-8. This measurement binds the enclave to specific AWS identity and access management policies, enabling attestation to verify that the enclave has appropriate permissions. For example, attestation policies could require that enclaves only run on instances with specific IAM roles that grant or restrict access to particular AWS resources. In the context of AWS Key Management Service integration, PCR3 enables KMS policies to restrict decryption operations to enclaves running with authorized roles.

PCR4 measures the specific EC2 instance identifier where the enclave is running. The computation follows the same pattern as PCR3, hashing an initial zero buffer concatenated with the SHA-384 hash of the instance ID string. This measurement binds the enclave to a specific physical instance, preventing attestation documents generated for one instance from being reused on another. While this binding provides some security properties, it also limits enclave portability, as attestation policies based on PCR4 will fail if the enclave migrates to a different instance.

PCR8 contains a measurement of the signing certificate used to sign the Enclave Image File, if signing is employed. The value is computed as the SHA-384 hash of an initial zero buffer concatenated with the SHA-384 hash of the signing certificate in DER format. This measurement enables attestation policies to restrict execution to enclaves signed by authorized parties, establishing a software supply chain control. Organizations can generate signing keys, sign approved enclave images, and configure attestation policies to only trust enclaves signed with their keys. This mechanism prevents unauthorized parties from creating enclaves that claim to implement specific functionality while actually executing malicious code.

The hierarchical hashing structure used in PCR computation, where each value is the hash of a zero buffer concatenated with another hash, serves to extend an initial state with new measurements. This approach allows for incremental updates to PCR values during multi-stage boot processes. However, security researchers have identified a limitation in the current implementation where the lack of domain separation between sections creates potential vulnerabilities. Because sections are simply concatenated before hashing, it may be possible in certain circumstances to move bytes between adjacent sections without changing the resulting PCR value. This weakness could theoretically enable attacks where an adversary constructs an EIF with different contents than expected but matching PCR measurements.

The absence of cryptographic binding between different PCRs represents another consideration for attestation policy design. Each PCR is computed independently, and the attestation document contains all six values without any cryptographic relationship between them. This design allows attestation policies to selectively enforce requirements on specific PCRs while ignoring others. For example, a policy might verify PCR1 and PCR2 to ensure correct kernel and application code without constraining the IAM role or instance ID. However, the independence of PCRs means that attestation policies must explicitly check all relevant measurements rather than relying on implicit relationships.

\subsection{Network Isolation and Communication Patterns}

The network architecture of Nitro Enclaves represents one of its most distinctive security features. Unlike traditional TEE implementations that allow enclaves to communicate over networks with appropriate encryption, Nitro Enclaves have no external network connectivity whatsoever. The enclave cannot establish TCP connections, send UDP packets, or perform any network operations that would reach beyond the physical instance. This absolute network isolation eliminates entire categories of attacks including data exfiltration through network channels, command and control communication with external attackers, and network-based reconnaissance of internal enclave state.

The vsock local socket mechanism provides the sole communication channel between enclaves and their parent instances. This virtual socket operates similarly to Unix domain sockets, providing bidirectional byte stream communication between processes. However, vsock operates at the hypervisor level rather than through the kernel's file system, maintaining isolation boundaries. The hypervisor mediates vsock connections, ensuring that enclaves can only communicate with their parent instances and not with other enclaves, other instances, or external networks. This strict mediation prevents enclave-to-enclave communication that could enable collusion or information sharing between separate security domains.

Applications must design communication patterns that accommodate the vsock-only constraint. The typical architecture involves the parent instance operating as a proxy that receives external requests through normal network interfaces, forwards relevant data to the enclave via vsock for processing, and returns results to external clients. This proxy pattern requires careful protocol design to maintain end-to-end security properties despite the additional hop through untrusted code in the parent instance. Sensitive data must remain encrypted during transit through the parent instance, with encryption keys accessible only within the enclave. The parent instance handles the network protocol overhead while the enclave focuses solely on confidential computation.

For decentralized AI applications, this architecture manifests in several patterns. User clients encrypt inference requests using public keys obtained from enclave attestation documents, ensuring that only the target enclave can decrypt the query. The encrypted request travels through public networks to the parent instance, which has no ability to decrypt or modify the contents. The parent instance forwards the encrypted request to the enclave via vsock, where the enclave decrypts using its private key, performs inference, encrypts the results, and returns encrypted output via vsock to the parent. The parent forwards the encrypted results to the client over the network. Throughout this flow, the actual query and results remain confidential despite traversing untrusted infrastructure.

The vsock connection operates as a standard socket API within the enclave, accessible through conventional socket programming interfaces. The enclave acts as a server listening on a vsock address, while the parent instance connects as a client. The hypervisor assigns a context identifier to the enclave that serves as its vsock address, enabling the parent to address messages to specific enclaves when multiple enclaves coexist on the same instance. This addressing scheme maintains isolation while enabling the parent to multiplex communication with concurrent enclaves.

Performance characteristics of vsock communication are generally favorable for typical workload patterns. The latency for round-trip communication between parent and enclave ranges from 100 microseconds to 500 microseconds depending on message size and system load. Bandwidth capabilities reach 10 to 20 gigabits per second, sufficient for transferring substantial data volumes. These performance metrics indicate that vsock communication does not represent a bottleneck for most AI inference workloads, where the computation time for inference typically exceeds the communication overhead by orders of magnitude.

The network isolation property simplifies security analysis by reducing the attack surface that must be considered during threat modeling. Network-based attacks including port scanning, service exploitation, and distributed denial of service are simply impossible against enclaves. The only attack surface accessible to external parties is the vsock interface, which can be carefully designed and audited. This constrained interface contrasts sharply with traditional server environments where every network service represents potential vulnerability. The elimination of network attack surface is particularly valuable in decentralized systems where nodes may face constant probing and attack attempts from malicious participants.

However, the network isolation also imposes constraints on certain application architectures. Services that require direct network access for functionality such as making HTTP requests to external APIs, participating in peer-to-peer networks, or receiving callbacks from external systems cannot operate within the isolation model. These applications must be redesigned to work through parent instance proxies or may be fundamentally incompatible with Nitro Enclaves. For AI inference specifically, the constraint typically poses minimal difficulty because inference is inherently a request-response operation that maps naturally to the vsock proxy pattern.

\subsection{Resource Management and Operational Considerations}

The operational characteristics of Nitro Enclaves affect deployment planning, capacity management, and runtime behavior. Understanding these practical considerations is essential for building production systems that leverage enclave capabilities effectively. The resource model of enclaves reflects the architectural decision to provide strong isolation through physical resource dedication rather than time-slicing or virtualized resource management.

CPU allocation to enclaves occurs at the granularity of complete virtual CPUs. When launching an enclave, operators specify how many vCPUs to allocate, with those vCPUs becoming unavailable to the parent instance. The minimum allocation is typically one vCPU, though two or more are recommended for non-trivial workloads to enable concurrent request handling and background operations. The maximum allocation depends on the parent instance type, as enclaves can consume at most the total vCPUs available minus those required for parent instance operation. For large AI inference workloads, allocating multiple vCPUs enables parallelism within model execution and improves throughput.

Memory allocation similarly operates through dedicated physical memory assignment. Operators specify the memory size in megabytes when launching the enclave, with that memory becoming inaccessible to the parent instance. The minimum practical allocation depends on the enclave's requirements, with typical AI inference workloads requiring gigabytes to accommodate model weights and activation tensors. The maximum allocation is constrained by the parent instance's total memory capacity minus the memory required for parent instance operation and any other concurrent enclaves. Instance types with large memory capacity such as memory-optimized families enable hosting models with tens of billions of parameters.

The relationship between Enclave Image File size and runtime memory requirements deserves careful consideration. The EIF size represents compressed and serialized data including the kernel, file systems, and application code. At runtime, these components must be decompressed and loaded into memory, with additional space required for heap allocation, stack growth, and temporary buffers. A practical rule of thumb suggests allocating at least four times the EIF size as runtime memory to accommodate decompression and execution requirements. Large language model deployments may require even more substantial allocations to hold model weights in memory during inference.

Enclave lifecycle management involves several states and transitions. Enclaves are created in a stopped state where resources are allocated but execution has not begun. Starting an enclave triggers the boot process including kernel initialization, init execution, and application startup. The enclave remains running until explicitly terminated or until the parent instance stops. Enclaves do not support pause or hibernation states, so any interruption requires complete termination and restart. This ephemeral nature means that enclaves must be designed to initialize quickly and maintain any necessary state externally through the parent instance or persistent storage services.

The lack of persistent storage within enclaves creates specific operational patterns. Enclaves cannot write to disk, access database files directly, or maintain any state that survives termination. Applications requiring persistence must implement external state management through APIs accessible via the parent instance. For AI model serving, this typically means loading model weights from encrypted storage on each enclave startup, decrypting them using keys obtained through attestation to AWS Key Management Service, and holding them in memory for the enclave's lifetime. This pattern ensures security by preventing model persistence in unencrypted form while requiring optimization to minimize startup time.

Multiple enclaves can coexist on a single parent instance, enabling various architectural patterns. Operators can run different AI models in separate enclaves, each with independent attestation and resource allocation. This multi-tenancy allows a single physical node to serve multiple clients or use cases while maintaining isolation. Alternatively, a single application might use multiple enclaves for different purposes such as model inference in one enclave and key management in another. The limit of four concurrent enclaves per instance represents a platform constraint that affects capacity planning for high-density deployments.

Instance type compatibility varies across the EC2 instance family portfolio. Most current generation instance types including general purpose, compute optimized, and memory optimized families support Nitro Enclaves. However, certain specialized instance types and older generations do not include enclave support. The minimum vCPU requirements differ by architecture, with Intel and AMD instances requiring at least four vCPUs while Graviton instances require only two. These requirements mean that very small instance types cannot host enclaves, affecting cost optimization for development and testing scenarios.

Regional availability of Nitro Enclaves spans most AWS regions but not all. Operators deploying global systems must verify that enclaves are supported in their target regions and potentially adjust deployment strategies for regions lacking support. The regional limitations reflect the underlying dependency on specific Nitro System hardware and software versions that may not have rolled out universally. Organizations requiring global presence may need to use alternative architectures or TEE technologies in regions where Nitro Enclaves remain unavailable.

The operational relationship between enclaves and parent instances creates dependencies that affect reliability and failure modes. Enclaves automatically terminate when the parent instance stops, whether due to planned shutdown or unexpected failure. This dependency means that maintaining enclave availability requires ensuring parent instance availability through appropriate instance management, health checking, and failover mechanisms. Enclaves cannot be migrated between instances while running, so any instance maintenance requires enclave termination and recreation. These operational characteristics influence high availability architecture design for production deployments.